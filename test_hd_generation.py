"""
Teste de GeraÃ§Ã£o de Imagens HD/4K
Testa diferentes resoluÃ§Ãµes para ver o que a RTX 5070 consegue gerar.
"""

import sys
import logging
import torch
from pathlib import Path
from typing import Dict, List, Tuple
import time

# Add src to path for imports
src_path = Path(__file__).parent / "src"
if str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))

from src.utils.config import Config
from src.utils.helpers import setup_logging, get_system_info
from src.models.sdxl_pipeline import SDXLPipeline

# Test resolutions
RESOLUTIONS = [
    ("1024x1024", 1024, 1024),    # Standard SDXL
    ("1280x720", 1280, 720),      # HD 720p
    ("1920x1080", 1920, 1080),    # Full HD 1080p
    ("2560x1440", 2560, 1440),    # QHD 1440p
    ("3840x2160", 3840, 2160),    # 4K UHD
]

TEST_PROMPT = """masterpiece, high quality, detailed, medieval fantasy castle on hilltop, 
dramatic sunset lighting, gothic architecture, stone walls, towers with flags, 
atmospheric clouds, cinematic composition, photorealistic"""

NEGATIVE_PROMPT = """blurry, low quality, pixelated, distorted, ugly, deformed, 
watermark, text, signature, artist name"""


def test_memory_requirements():
    """Testa os requisitos de memÃ³ria da GPU."""
    print("ğŸ” Verificando memÃ³ria GPU disponÃ­vel...")
    
    if torch.cuda.is_available():
        device = torch.cuda.current_device()
        gpu_memory = torch.cuda.get_device_properties(device).total_memory
        gpu_memory_gb = gpu_memory / (1024**3)
        
        free_memory = torch.cuda.mem_get_info()[0]
        free_memory_gb = free_memory / (1024**3)
        
        print(f"ğŸ“Š GPU: {torch.cuda.get_device_name(device)}")
        print(f"ğŸ’¾ MemÃ³ria Total: {gpu_memory_gb:.1f} GB")
        print(f"ğŸ†“ MemÃ³ria Livre: {free_memory_gb:.1f} GB")
        
        return gpu_memory_gb, free_memory_gb
    else:
        print("âŒ CUDA nÃ£o disponÃ­vel")
        return 0, 0


def estimate_vram_usage(width: int, height: int) -> float:
    """
    Estima o uso de VRAM para uma resoluÃ§Ã£o especÃ­fica.
    Baseado em fÃ³rmulas empÃ­ricas para SDXL.
    """
    # Base VRAM para SDXL (modelo + overhead)
    base_vram = 6.5  # GB
    
    # VRAM adicional baseado na resoluÃ§Ã£o
    pixels = width * height
    base_pixels = 1024 * 1024  # ResoluÃ§Ã£o padrÃ£o SDXL
    
    # Fator de escala para VRAM (nÃ£o linear)
    scale_factor = (pixels / base_pixels) ** 0.8
    additional_vram = (scale_factor - 1) * 2.5  # GB
    
    return base_vram + additional_vram


def test_resolution(
    pipeline: SDXLPipeline,
    name: str,
    width: int,
    height: int,
    output_dir: Path
) -> Dict:
    """Testa uma resoluÃ§Ã£o especÃ­fica."""
    print(f"\nğŸ¯ Testando resoluÃ§Ã£o: {name} ({width}x{height})")
    
    # Estimar VRAM necessÃ¡rio
    estimated_vram = estimate_vram_usage(width, height)
    print(f"ğŸ“Š VRAM estimado necessÃ¡rio: {estimated_vram:.1f} GB")
    
    # Limpar cache da GPU
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    
    start_time = time.time()
    success = False
    error_msg = None
    output_path = None
    
    try:
        # Gerar imagem
        print(f"ğŸ¨ Gerando imagem {width}x{height}...")
        
        result = pipeline.generate_image(
            prompt=TEST_PROMPT,
            negative_prompt=NEGATIVE_PROMPT,
            width=width,
            height=height,
            num_inference_steps=50,  # Reduzido para teste mais rÃ¡pido
            guidance_scale=8.5
        )
        
        if result and result.get("success"):
            # Salvar imagem
            output_path = output_dir / f"test_{name.lower().replace('x', '_')}.png"
            
            # A imagem estÃ¡ em result["image"]
            image = result["image"]
            image.save(output_path)
            
            success = True
            print(f"âœ… Sucesso! Imagem salva em: {output_path}")
            
        else:
            error_msg = result.get("error", "Erro desconhecido na geraÃ§Ã£o")
            print(f"âŒ Falha: {error_msg}")
            
    except Exception as e:
        error_msg = str(e)
        print(f"âŒ Erro durante geraÃ§Ã£o: {error_msg}")
        
        # Verificar se Ã© erro de VRAM
        if "out of memory" in error_msg.lower() or "cuda" in error_msg.lower():
            print("ğŸ’¾ Erro de memÃ³ria GPU detectado")
    
    end_time = time.time()
    generation_time = end_time - start_time
    
    # Verificar memÃ³ria apÃ³s geraÃ§Ã£o
    if torch.cuda.is_available():
        torch.cuda.synchronize()
        memory_used = torch.cuda.max_memory_allocated() / (1024**3)
        print(f"ğŸ“Š MemÃ³ria GPU usada: {memory_used:.1f} GB")
        torch.cuda.reset_peak_memory_stats()
    else:
        memory_used = 0
    
    return {
        "resolution": name,
        "width": width,
        "height": height,
        "success": success,
        "error": error_msg,
        "generation_time": generation_time,
        "estimated_vram": estimated_vram,
        "actual_vram": memory_used,
        "output_path": str(output_path) if output_path else None
    }


def main():
    """FunÃ§Ã£o principal do teste."""
    print("ğŸ° Medieval Deck - Teste de GeraÃ§Ã£o HD/4K")
    print("=" * 50)
    
    # Setup logging
    setup_logging(level="INFO")
    
    # Verificar sistema
    print("\nğŸ” InformaÃ§Ãµes do Sistema:")
    system_info = get_system_info()
    print(f"GPU: {system_info.get('cuda', {}).get('device_name', 'N/A')}")
    
    # Verificar memÃ³ria
    total_gpu_memory, free_gpu_memory = test_memory_requirements()
    
    if total_gpu_memory < 12:
        print(f"âš ï¸ Aviso: GPU com {total_gpu_memory:.1f} GB pode ter limitaÃ§Ãµes para 4K")
    
    # Criar diretÃ³rio de output
    output_dir = Path("test_hd_outputs")
    output_dir.mkdir(exist_ok=True)
    
    try:
        # Inicializar configuraÃ§Ã£o
        config = Config()
        
        # Inicializar pipeline SDXL
        print("\nğŸš€ Inicializando pipeline SDXL...")
        pipeline = SDXLPipeline(
            model_id=config.ai.model_id,
            refiner_id=config.ai.refiner_id,
            enable_refiner=False,  # Desabilitar refiner para economizar VRAM
            device=config.get_device(),
            cache_dir=str(config.assets_cache_dir),
            memory_efficient=True
        )
        
        print("âœ… Pipeline inicializado com sucesso!")
        
        # Testar resoluÃ§Ãµes
        results = []
        for name, width, height in RESOLUTIONS:
            # Verificar se vale a pena tentar
            estimated_vram = estimate_vram_usage(width, height)
            if estimated_vram > free_gpu_memory * 1.2:  # Margem de seguranÃ§a
                print(f"\nâ­ï¸ Pulando {name}: VRAM estimado ({estimated_vram:.1f} GB) > disponÃ­vel ({free_gpu_memory:.1f} GB)")
                continue
            
            result = test_resolution(pipeline, name, width, height, output_dir)
            results.append(result)
            
            # Se falhou por memÃ³ria, parar testes maiores
            if not result["success"] and result.get("error") and "memory" in result["error"].lower():
                print("\nğŸ›‘ Parando testes - limite de memÃ³ria atingido")
                break
        
        # RelatÃ³rio final
        print("\n" + "=" * 50)
        print("ğŸ“Š RELATÃ“RIO FINAL")
        print("=" * 50)
        
        successful_resolutions = []
        for result in results:
            status = "âœ… SUCESSO" if result["success"] else "âŒ FALHA"
            print(f"\n{result['resolution']}: {status}")
            print(f"  Tempo: {result['generation_time']:.1f}s")
            print(f"  VRAM estimado: {result['estimated_vram']:.1f} GB")
            print(f"  VRAM usado: {result['actual_vram']:.1f} GB")
            
            if result["success"]:
                successful_resolutions.append(result["resolution"])
                print(f"  Arquivo: {result['output_path']}")
            else:
                print(f"  Erro: {result['error']}")
        
        # RecomendaÃ§Ãµes
        print(f"\nğŸ¯ RECOMENDAÃ‡Ã•ES:")
        if successful_resolutions:
            max_resolution = successful_resolutions[-1]
            print(f"âœ… ResoluÃ§Ã£o mÃ¡xima suportada: {max_resolution}")
            
            if "4K" in max_resolution or "3840x2160" in max_resolution:
                print("ğŸ”¥ Excelente! Sua GPU suporta geraÃ§Ã£o 4K!")
            elif "1920x1080" in max_resolution:
                print("ğŸ‘ Ã“timo! Sua GPU suporta Full HD!")
            elif "1280x720" in max_resolution:
                print("ğŸ‘Œ Sua GPU suporta HD 720p")
            
            print(f"ğŸ’¡ Para melhor performance, use: {successful_resolutions[-1]}")
            print(f"ğŸ’¡ Para qualidade mÃ¡xima, use: {successful_resolutions[-1]} com refiner")
        else:
            print("âŒ Nenhuma resoluÃ§Ã£o HD foi suportada")
            print("ğŸ’¡ Tente reduzir outras configuraÃ§Ãµes ou usar modo memory_efficient")
        
        print(f"\nğŸ“ Imagens geradas salvas em: {output_dir.absolute()}")
        
    except Exception as e:
        print(f"\nâŒ Erro fatal no teste: {e}")
        logging.error(f"Erro no teste HD: {e}", exc_info=True)
        return 1
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
