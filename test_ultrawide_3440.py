"""
Teste direto de gera√ß√£o 3440x1440 (Ultrawide QHD)
Testa a maior resolu√ß√£o ultrawide que a RTX 5070 consegue gerar.
"""

import sys
import logging
import torch
from pathlib import Path
import time

# Add src to path for imports
src_path = Path(__file__).parent / "src"
if str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))

from src.utils.config import Config
from src.utils.helpers import setup_logging
from src.models.sdxl_pipeline import SDXLPipeline

# Test ultrawide resolution
WIDTH = 3440
HEIGHT = 1440

TEST_PROMPT = """masterpiece, high quality, detailed, medieval fantasy castle on hilltop, 
dramatic sunset lighting, gothic architecture, stone walls, towers with flags, 
atmospheric clouds, cinematic composition, wide panoramic view"""

NEGATIVE_PROMPT = """blurry, low quality, pixelated, distorted, ugly, deformed, 
people, person, human, character, figure"""


def test_ultrawide():
    """Teste de gera√ß√£o ultrawide 3440x1440."""
    print("üè∞ Medieval Deck - Teste Ultrawide 3440x1440")
    print("=" * 50)
    
    # Setup logging
    setup_logging(level="INFO")
    
    # Verificar GPU
    if torch.cuda.is_available():
        device = torch.cuda.current_device()
        gpu_name = torch.cuda.get_device_name(device)
        gpu_memory = torch.cuda.get_device_properties(device).total_memory / (1024**3)
        free_memory = torch.cuda.mem_get_info()[0] / (1024**3)
        
        print(f"üî• GPU: {gpu_name}")
        print(f"üíæ Mem√≥ria Total: {gpu_memory:.1f} GB")
        print(f"üÜì Mem√≥ria Livre: {free_memory:.1f} GB")
    else:
        print("‚ùå CUDA n√£o dispon√≠vel")
        return 1
    
    # Estimar VRAM necess√°rio
    pixels = WIDTH * HEIGHT
    base_pixels = 1024 * 1024
    scale_factor = (pixels / base_pixels) ** 0.8
    estimated_vram = 6.5 + (scale_factor - 1) * 2.5
    
    print(f"\nüìä Resolu√ß√£o: {WIDTH}x{HEIGHT}")
    print(f"üìä Pixels: {pixels:,}")
    print(f"üìä VRAM estimado: {estimated_vram:.1f} GB")
    
    if estimated_vram > free_memory * 0.9:
        print(f"‚ö†Ô∏è Aviso: VRAM estimado ({estimated_vram:.1f} GB) pr√≥ximo do limite ({free_memory:.1f} GB)")
    
    # Criar diret√≥rio de output
    output_dir = Path("test_ultrawide")
    output_dir.mkdir(exist_ok=True)
    
    try:
        # Inicializar configura√ß√£o
        config = Config()
        
        # Inicializar pipeline
        print(f"\nüöÄ Inicializando pipeline SDXL...")
        pipeline = SDXLPipeline(
            model_id=config.ai.model_id,
            refiner_id=config.ai.refiner_id,
            enable_refiner=False,  # Desabilitar para economizar VRAM
            device=config.get_device(),
            cache_dir=str(config.assets_cache_dir),
            memory_efficient=True
        )
        
        print("‚úÖ Pipeline inicializado!")
        
        # Limpar cache
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            
            # Mem√≥ria antes
            memory_before = torch.cuda.memory_allocated() / (1024**3)
            print(f"\nüìä Mem√≥ria GPU antes: {memory_before:.1f} GB")
        
        print(f"\nüé® Gerando imagem {WIDTH}x{HEIGHT}...")
        print("‚ö° Usando 25 steps para otimizar velocidade...")
        
        start_time = time.time()
        
        # Gerar imagem
        image = pipeline.generate_image(
            prompt=TEST_PROMPT,
            negative_prompt=NEGATIVE_PROMPT,
            width=WIDTH,
            height=HEIGHT,
            num_inference_steps=25,  # Reduzido para economizar VRAM
            guidance_scale=8.0  # Ligeiramente reduzido
        )
        
        end_time = time.time()
        generation_time = end_time - start_time
        
        # Salvar imagem
        output_path = output_dir / f"medieval_castle_ultrawide_{WIDTH}x{HEIGHT}.png"
        image.save(output_path)
        
        # Verificar mem√≥ria ap√≥s
        if torch.cuda.is_available():
            torch.cuda.synchronize()
            memory_after = torch.cuda.memory_allocated() / (1024**3)
            memory_peak = torch.cuda.max_memory_allocated() / (1024**3)
            
            print(f"\n‚úÖ SUCESSO! Imagem ultrawide gerada!")
            print(f"üìÅ Arquivo: {output_path}")
            print(f"‚è±Ô∏è Tempo: {generation_time:.1f}s")
            print(f"üìä Mem√≥ria GPU depois: {memory_after:.1f} GB")
            print(f"üìä Mem√≥ria GPU pico: {memory_peak:.1f} GB")
            
            # Calcular efici√™ncia
            mpixels = (WIDTH * HEIGHT) / 1000000
            speed = mpixels / generation_time
            print(f"üöÄ Velocidade: {speed:.2f} megapixels/segundo")
            
            torch.cuda.reset_peak_memory_stats()
        
        print(f"\nüéØ RESULTADO:")
        print(f"‚úÖ RTX 5070 consegue gerar {WIDTH}x{HEIGHT} (Ultrawide QHD)!")
        print(f"üí° Tempo razo√°vel: {generation_time:.1f}s para {mpixels:.1f} megapixels")
        print(f"üí° Use esta resolu√ß√£o para paisagens cinematogr√°ficas!")
        
        return 0
        
    except Exception as e:
        print(f"\n‚ùå Erro durante gera√ß√£o: {e}")
        
        # Verificar se √© erro de VRAM
        if "out of memory" in str(e).lower() or "cuda" in str(e).lower():
            print("üíæ Erro de mem√≥ria GPU!")
            print("üí° Tente:")
            print("  - Menos steps (15-20)")
            print("  - Guidance scale menor (7.0)")
            print("  - Resolu√ß√£o menor (2560x1440)")
        
        return 1
    
    finally:
        # Cleanup
        if torch.cuda.is_available():
            torch.cuda.empty_cache()


if __name__ == "__main__":
    sys.exit(test_ultrawide())
